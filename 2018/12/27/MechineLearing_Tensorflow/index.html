<!DOCTYPE html><html class="theme-next mist use-motion" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet"><link href="/css/main.css?v=5.1.4" rel="stylesheet"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon-16x16.png?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x21.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png?v=5.1.4"><link rel="mask-icon" href="/images/favicon-16x16.png?v=5.1.4" color="#222"><meta name="keywords" content="Python,"><meta name="description" content="摘要跟着网课学习神经网络，内容开始渐渐变得玄学起来。神经网络是一门玄学基础课，没有明确的理论依据，是经验总结生成的数学公式，还有模型是通过机器学习学习机器学习模型设计的。为什么能达到这么高的准确率，怎样调参数能达到更高的准确率？似乎谁也说不清楚，只能先调为敬。结果说明一切。他的公式就和牛顿第一定律一样，是属于经验总结而成的数学运算，不一定适用所有情况。对应教学视频：人工智能实践：Tensorflo"><meta name="keywords" content="Python"><meta property="og:type" content="article"><meta property="og:title" content="机器学习——神经网络笔记"><meta property="og:url" content="https://sika0819.top/2018/12/27/MechineLearing_Tensorflow/index.html"><meta property="og:site_name" content="Sika的养虫小窝"><meta property="og:description" content="摘要跟着网课学习神经网络，内容开始渐渐变得玄学起来。神经网络是一门玄学基础课，没有明确的理论依据，是经验总结生成的数学公式，还有模型是通过机器学习学习机器学习模型设计的。为什么能达到这么高的准确率，怎样调参数能达到更高的准确率？似乎谁也说不清楚，只能先调为敬。结果说明一切。他的公式就和牛顿第一定律一样，是属于经验总结而成的数学运算，不一定适用所有情况。对应教学视频：人工智能实践：Tensorflo"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="https://sika0819.top/images/pasted-40.png"><meta property="og:image" content="https://sika0819.top/images/pasted-41.png"><meta property="og:image" content="https://sika0819.top/images/pasted-42.png"><meta property="og:image" content="https://sika0819.top/images/pasted-43.png"><meta property="og:image" content="https://sika0819.top/images/pasted-44.png"><meta property="og:image" content="https://sika0819.top/images/pasted-45.png"><meta property="og:image" content="https://sika0819.top/images/pasted-46.png"><meta property="og:image" content="https://sika0819.top/images/pasted-47.png"><meta property="og:updated_time" content="2019-05-15T07:13:14.579Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="机器学习——神经网络笔记"><meta name="twitter:description" content="摘要跟着网课学习神经网络，内容开始渐渐变得玄学起来。神经网络是一门玄学基础课，没有明确的理论依据，是经验总结生成的数学公式，还有模型是通过机器学习学习机器学习模型设计的。为什么能达到这么高的准确率，怎样调参数能达到更高的准确率？似乎谁也说不清楚，只能先调为敬。结果说明一切。他的公式就和牛顿第一定律一样，是属于经验总结而成的数学运算，不一定适用所有情况。对应教学视频：人工智能实践：Tensorflo"><meta name="twitter:image" content="https://sika0819.top/images/pasted-40.png"><script id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",version:"5.1.4",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="https://sika0819.top/2018/12/27/MechineLearing_Tensorflow/"><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><title>机器学习——神经网络笔记 | Sika的养虫小窝</title></head><script src="/js/src/crash_cheat.js"></script><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Sika的养虫小窝</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle"></p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span><span class="btn-bar"></span><span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-首页"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-归档"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://sika0819.top/2018/12/27/MechineLearing_Tensorflow/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="赵思佳"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Sika的养虫小窝"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">机器学习——神经网络笔记</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-27T15:46:00+08:00">2018-12-27</time></span> <span class="post-category"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>跟着网课学习神经网络，内容开始渐渐变得玄学起来。神经网络是一门玄学基础课，没有明确的理论依据，是经验总结生成的数学公式，还有模型是通过机器学习学习机器学习模型设计的。为什么能达到这么高的准确率，怎样调参数能达到更高的准确率？似乎谁也说不清楚，只能先调为敬。结果说明一切。他的公式就和牛顿第一定律一样，是属于经验总结而成的数学运算，不一定适用所有情况。<br>对应教学视频：<a href="https://www.icourse163.org/learn/PKU-1002536002" target="_blank" rel="noopener">人工智能实践：Tensorflow笔记</a><br><a id="more"></a></p><h1 id="基于Tensorflow的NN"><a href="#基于Tensorflow的NN" class="headerlink" title="基于Tensorflow的NN"></a>基于Tensorflow的NN</h1><p>用张量表示数据，用计算图搭建神经网络，用会话执行计算图，优化线上的权重（参数）得到模型。</p><h2 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h2><p><strong>多维数组（列表） 阶：张量的维数</strong></p><table><thead><tr><th>维数</th><th style="text-align:center">阶</th><th style="text-align:center">名字</th><th style="text-align:center">例子</th></tr></thead><tbody><tr><td>0-D</td><td style="text-align:center">0</td><td style="text-align:center">标量scalar</td><td style="text-align:center">s=1 2 3</td></tr><tr><td>1-D</td><td style="text-align:center">1</td><td style="text-align:center">向量vector</td><td style="text-align:center">v=[1,2,3]</td></tr><tr><td>2-D</td><td style="text-align:center">2</td><td style="text-align:center">矩阵matrix</td><td style="text-align:center">m=[[1,2,3],[4,5,6],[7,8,9]]</td></tr><tr><td>n-D</td><td style="text-align:center">n</td><td style="text-align:center">张量tensor</td><td style="text-align:center">t=[[[[……]]]</td></tr></tbody></table><p><strong>张量其实就是0阶到n阶的多维数组（列表）</strong></p><h2 id="数据类型："><a href="#数据类型：" class="headerlink" title="数据类型："></a>数据类型：</h2><p><strong>tf.float32,tf.int32等 例：</strong><br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a=tf.constant([<span class="number">1.0</span>,<span class="number">2.0</span>])</span><br><span class="line">b=tf.constant([<span class="number">3.0</span>,<span class="number">4.0</span>])</span><br><span class="line">result=a+b</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><p></p><p>结果：Tensor(“add:0”, shape=(2,), dtype=float32)<br>输出结果是张量的名字，其实就是一个计算图。</p><h2 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h2><p>搭建神经网络的运算过程，只搭建，不运算<br><img src="/images/pasted-40.png" alt="upload successful"><br><strong>例：y=XW=x1<em>w1+x2</em>w2</strong><br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x=tf.constant([[<span class="number">1.0</span>,<span class="number">2.0</span>]])</span><br><span class="line">w=tf.constant([[<span class="number">3.0</span>],[<span class="number">4.0</span>]])</span><br><span class="line">y=tf.matmul(x,w)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><p></p><p><strong>结果：Tensor(“MatMul_1:0”, shape=(1, 1), dtype=float32)</strong></p><h2 id="会话（Sesson）"><a href="#会话（Sesson）" class="headerlink" title="会话（Sesson）"></a>会话（Sesson）</h2><p>执行计算图中的节点运算<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x=tf.constant([[<span class="number">1.0</span>,<span class="number">2.0</span>]])</span><br><span class="line">w=tf.constant([[<span class="number">3.0</span>],[<span class="number">4.0</span>]])</span><br><span class="line">y=tf.matmul(x,w)</span><br><span class="line">print(y)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(y))</span><br></pre></td></tr></table></figure><p></p><p>结果：[[11.]]</p><h1 id="神经网络实现步骤"><a href="#神经网络实现步骤" class="headerlink" title="神经网络实现步骤"></a>神经网络实现步骤</h1><ol><li>准备数据集，提取特征，作为输入喂给神经网络（Neural Network,NN)</li><li>搭建NN结构，从输入到输出（先搭建计算图，再用会话执行）<br>（NN向前传播算法→计算输出)</li><li>大量特征数据喂给NN,迭代优化NN参数<br>（NN反向传播算法→优化参数训练模型）</li><li>使用训练号的模型预测和分类</li></ol><h1 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h1><h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w=tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">3</span>],stddev=<span class="number">2</span>,mean=<span class="number">0</span>,seed=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><ul><li><p><strong>tf.random_normal</strong>代表符合正太分布的随机数。[2,3]代表产生一个2 * 3的矩阵，mean=0代表均值为0</p></li><li><p><strong>tf.truncated_normal</strong> 是去掉过大偏离点的正太分布</p></li><li><p><strong>tf.random_uniform()</strong> 是平均分布</p></li><li><p><strong>tf.zeros</strong>全零数组</p></li><li><p><strong>tf.one</strong>全一数组</p></li><li><p><strong>tf.fill</strong>全定值数组</p></li><li><p><strong>tf.constant</strong>直接给值</p></li></ul><h2 id="前向传播搭建模型，实现推理"><a href="#前向传播搭建模型，实现推理" class="headerlink" title="前向传播搭建模型，实现推理"></a>前向传播搭建模型，实现推理</h2><p>eg. 生成一批零件，将体积x<sub>1</sub>和重量x<sub>2</sub>为特征输入NN，通过NN后输出一个数组<br><img src="/images/pasted-41.png" alt="upload successful"></p><h2 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h2><p>x是一个1X4的矩阵，表示一次输入一组特征。<br>W<sub>前节点编号,后节点编号</sub> <sup>(层数)</sup> 为待优化的参数（权重矩阵）。<br>W<sup>(1)</sup> 是一个2*3的矩阵<br>$$<br>W^{(1)}=\left[<br>\begin{matrix}<br>W_{1,1}^{(1)}&amp;W_{1,2}^{(2)}&amp;W_{1,3}^{(2)}\<br>W_{2,1}^{(1)}&amp;W_{2,2}^{(2)}&amp;W_{2,3}^{(2)}\<br>\end{matrix}<br>\right]<br>$$</p><p>a<sup>(1)</sup>=[a<sub>11</sub>,a<sub>12</sub>,a<sub>13</sub>]为1X3矩阵=XW<sup>(1)</sup></p><p>W<sup>(2)</sup>为3X1矩阵<br>$$<br>W^{(2)}=\left[<br>\begin{matrix}<br>W_{1,1}^{(2)}\<br>W_{2,1}^{(2)}\<br>W_{3,1}^{(2)}\<br>\end{matrix}<br>\right]<br>$$<br>写成tensorflow的形式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a=tf.matmul(X,W1)</span><br><span class="line">y=tf.matmul（a,W2)</span><br></pre></td></tr></table></figure><h2 id="变量初始化（with结构）"><a href="#变量初始化（with结构）" class="headerlink" title="变量初始化（with结构）"></a>变量初始化（with结构）</h2><p>用会话实现计算图节点运算：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">sess.run()</span><br></pre></td></tr></table></figure><p></p><h2 id="变量初始化（使用初始化函数）"><a href="#变量初始化（使用初始化函数）" class="headerlink" title="变量初始化（使用初始化函数）"></a>变量初始化（使用初始化函数）</h2><p>在sess.run函数中用tf.global_variables_intializer()：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">init_op=tf.global_variables_initiablizer()</span><br><span class="line">sess.run(init_op)</span><br></pre></td></tr></table></figure><p></p><h2 id="图节点运算"><a href="#图节点运算" class="headerlink" title="图节点运算"></a>图节点运算</h2><p>在sess.run函数中写入待运算的节点<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(y)</span><br></pre></td></tr></table></figure><p></p><p>实际运用中可以输入一组或多组数据。<br>用tf.placeholder占位，在sess.run函数中用feed_dict喂数据</p><ul><li><p>喂一组数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x=tf.placeholder(tf.float32,shape=(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">sess.run(y,feed_dict=&#123;x:[[<span class="number">0.5</span>,<span class="number">0.6</span>]]&#125;)</span><br></pre></td></tr></table></figure></li><li><p>喂多组数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x=tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">2</span>))</span><br><span class="line">sess.run(y,feed_dict=&#123;x:[[<span class="number">0.1</span>,<span class="number">0.2</span>],[<span class="number">0.2</span>,<span class="number">0.3</span>],[<span class="number">0.3</span>,<span class="number">0.4</span>],[<span class="number">0.4</span>,<span class="number">0.5</span>]]&#125;)</span><br></pre></td></tr></table></figure></li></ul><h2 id="前向传播代码示例"><a href="#前向传播代码示例" class="headerlink" title="前向传播代码示例"></a>前向传播代码示例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="comment">#两层简单神经网络（全连接）</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment">#定义输入和参数</span></span><br><span class="line">x=tf.constant([[<span class="number">0.7</span>,<span class="number">0.5</span>]])</span><br><span class="line">w1=tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">3</span>],stddev=<span class="number">1</span>,seed=<span class="number">1</span>))</span><br><span class="line">w2=tf.Variable(tf.random_normal([<span class="number">3</span>,<span class="number">1</span>],stddev=<span class="number">1</span>,seed=<span class="number">1</span>))</span><br><span class="line"><span class="comment">#定义前向传播</span></span><br><span class="line">a=tf.matmul(x,w1);</span><br><span class="line">y=tf.matmul(a,w2);</span><br><span class="line"><span class="comment">#用会话计算结果</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">	init_op=tf.global_variables_initializer()</span><br><span class="line">	sess.run(init_op)</span><br><span class="line">	print(<span class="string">"y is :"</span>,sess.run(y))</span><br></pre></td></tr></table></figure><p>结果：y is : [[3.0904665]]</p><h1 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h1><p>训练模型参数，在所有参数上用梯度下降使NN模型在训练数据上的损失最小</p><h2 id="反向传播训练方法："><a href="#反向传播训练方法：" class="headerlink" title="反向传播训练方法："></a>反向传播训练方法：</h2><p>以减小loss值喂优化目标<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)</span><br><span class="line">train_step=tf.train.MomentumOptimizer(learning_rate).minimize(loss)</span><br><span class="line">train_step=tf.train.AdamOptimizer(learning_rate).minimize(loss)</span><br></pre></td></tr></table></figure><p></p><h1 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h1><ul><li>relu<br><img src="/images/pasted-42.png" alt="upload successful"><br><code>tf.nn.relu()</code></li><li>sigmoid<br><img src="/images/pasted-43.png" alt="upload successful"><br><code>tf.nn.sigmoid()</code></li><li>tanh<br><img src="/images/pasted-44.png" alt="upload successful"><br><code>tf.nn.tanh()</code></li></ul><h1 id="损失函数-loss"><a href="#损失函数-loss" class="headerlink" title="损失函数(loss)"></a>损失函数(loss)</h1><p>预测值(y)与已知答案(y_)的差距</p><h2 id="均方误差MSE"><a href="#均方误差MSE" class="headerlink" title="均方误差MSE"></a>均方误差MSE</h2><p>$$<br>MSE(y_,y)=\frac{\sum_{i=1}^n(y-y\underline{})^2}{n}<br>$$<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss=tf.reduce_mean(tf.square(y_,-y))</span><br></pre></td></tr></table></figure><p></p><h2 id="交叉熵Cross-Entropy"><a href="#交叉熵Cross-Entropy" class="headerlink" title="交叉熵Cross Entropy"></a>交叉熵Cross Entropy</h2><p>$$<br>H(y\underline{},y)=-\sum y\underline{}*log(y)<br>$$<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ce=-tf.reduce_mean(y_*tf.log(tf.clip__by_value(y,<span class="number">1e-12</span>,<span class="number">1</span>)))</span><br></pre></td></tr></table></figure><p></p><h1 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h1><p>决定参数每次更新的幅度<br>$$<br>w_{n+1}=w_n-learningrate▽<br>$$</p><h2 id="指数衰减学习率"><a href="#指数衰减学习率" class="headerlink" title="指数衰减学习率"></a>指数衰减学习率</h2><p>$$<br>learning\underline{}rate=LEARNING\underline{}RATE\underline{}BASE*LEARNING\underline{}RATE\underline{}DECAY^{\frac{globalstep}{LEARNING\underline{}RATE\underline{}STEP}}<br>$$<br>代码示例：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">LEARNING_RATE_BASE=<span class="number">0.1</span></span><br><span class="line">LEARNING_RATE_DECAY=<span class="number">0.99</span></span><br><span class="line">LEARNING_RATE_STEP=<span class="number">1</span></span><br><span class="line">global_step=tf.Variable(<span class="number">0</span>,trainable=<span class="keyword">False</span>)</span><br><span class="line">learning_rate=tf.train.exponential_decay(LEARNING_RATE_BASE,global_step,LEARNING_RATE_STEP,LEARNING_RATE_DECAY,staircase=<span class="keyword">True</span>)</span><br><span class="line">w=tf.Variable(tf.constant(<span class="number">5</span>,dtype=tf.float32))</span><br><span class="line">loss=tf.square(w+<span class="number">1</span>)</span><br><span class="line">train_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op=tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">40</span>):</span><br><span class="line">        sess.run(train_step)</span><br><span class="line">        learning_rate_val=sess.run(learning_rate)</span><br><span class="line">        global_step_val=sess.run(global_step)</span><br><span class="line">        w_val=sess.run(w)</span><br><span class="line">        loss_val=sess.run(loss)</span><br><span class="line">        print(i,global_step_val,w_val,loss_val,learning_rate_val)</span><br></pre></td></tr></table></figure><p></p><p>打印结果:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">0 1 3.8 23.04 0.099</span><br><span class="line">1 2 2.8495998 14.819419 0.09801</span><br><span class="line">2 3 2.0950012 9.579033 0.0970299</span><br><span class="line">3 4 1.494386 6.2219615 0.096059605</span><br><span class="line">4 5 1.0151665 4.060896 0.09509901</span><br><span class="line">5 6 0.6318858 2.6630511 0.094148025</span><br><span class="line">6 7 0.32460818 1.7545869 0.09320655</span><br><span class="line">7 8 0.077683866 1.1614027 0.09227448</span><br><span class="line">8 9 -0.121201575 0.77228665 0.09135174</span><br><span class="line">9 10 -0.2817611 0.5158671 0.09043822</span><br><span class="line">10 11 -0.4116736 0.34612796 0.089533836</span><br><span class="line">11 12 -0.51702386 0.23326595 0.0886385</span><br><span class="line">12 13 -0.60264444 0.15789144 0.08775212</span><br><span class="line">13 14 -0.672382 0.107333556 0.0868746</span><br><span class="line">14 15 -0.7293054 0.07327557 0.08600585</span><br><span class="line">15 16 -0.77586806 0.050235126 0.085145794</span><br><span class="line">16 17 -0.81403583 0.03458267 0.084294334</span><br><span class="line">17 18 -0.8453873 0.023905093 0.08345139</span><br><span class="line">18 19 -0.8711926 0.016591353 0.08261688</span><br><span class="line">19 20 -0.8924759 0.011561431 0.08179071</span><br><span class="line">20 21 -0.9100648 0.008088337 0.080972806</span><br><span class="line">21 22 -0.92462945 0.00568072 0.08016308</span><br><span class="line">22 23 -0.93671334 0.0040052016 0.079361446</span><br><span class="line">23 24 -0.9467584 0.002834669 0.07856783</span><br><span class="line">24 25 -0.95512456 0.0020138053 0.077782154</span><br><span class="line">25 26 -0.9621056 0.0014359877 0.077004336</span><br><span class="line">26 27 -0.96794164 0.0010277383 0.076234296</span><br><span class="line">27 28 -0.9728295 0.00073823496 0.07547195</span><br><span class="line">28 29 -0.97693074 0.0005321909 0.07471723</span><br><span class="line">29 30 -0.9803781 0.0003850193 0.073970065</span><br><span class="line">30 31 -0.98328096 0.0002795264 0.073230356</span><br><span class="line">31 32 -0.98572963 0.00020364333 0.07249805</span><br><span class="line">32 33 -0.9877988 0.00014886903 0.071773075</span><br><span class="line">33 34 -0.98955023 0.00010919763 0.071055345</span><br><span class="line">34 35 -0.9910353 8.036616e-05 0.0703448</span><br><span class="line">35 36 -0.9922965 5.9343653e-05 0.069641344</span><br><span class="line">36 37 -0.99336946 4.396406e-05 0.06894493</span><br><span class="line">37 38 -0.99428374 3.2675678e-05 0.068255484</span><br><span class="line">38 39 -0.9950641 2.436331e-05 0.06757293</span><br><span class="line">39 40 -0.9957312 1.8222867e-05 0.066897206</span><br></pre></td></tr></table></figure><p></p><h2 id="反向传播代码示例"><a href="#反向传播代码示例" class="headerlink" title="反向传播代码示例:"></a>反向传播代码示例:</h2><p>喂一组数据:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="comment">#两层简单神经网络（全连接）</span></span><br><span class="line"><span class="comment">#反向传播喂一组数据</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment">#定义输入和参数</span></span><br><span class="line"><span class="comment">#用placeholder实现输入定义(sess.run中喂一组数据）</span></span><br><span class="line">x=tf.placeholder(tf.float32,shape=(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">w1=tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">3</span>],stddev=<span class="number">1</span>,seed=<span class="number">1</span>))</span><br><span class="line">w2=tf.Variable(tf.random_normal([<span class="number">3</span>,<span class="number">1</span>],stddev=<span class="number">1</span>,seed=<span class="number">1</span>))</span><br><span class="line"><span class="comment">#定义前向传播</span></span><br><span class="line">a=tf.matmul(x,w1);</span><br><span class="line">y=tf.matmul(a,w2);</span><br><span class="line"><span class="comment">#用会话计算结果</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">	init_op=tf.global_variables_initializer()</span><br><span class="line">	sess.run(init_op)</span><br><span class="line">	print(<span class="string">"y is :"</span>,sess.run(y,feed_dict=&#123;x:[[<span class="number">0.7</span>,<span class="number">0.5</span>]]&#125;))</span><br></pre></td></tr></table></figure><p></p><p>结果：y is : [[3.0904665]]</p><p>#喂n组数据<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="comment">#两层简单神经网络（全连接）</span></span><br><span class="line"><span class="comment">#反向传播喂n组数据</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment">#定义输入和参数</span></span><br><span class="line"><span class="comment">#用placeholder实现输入定义(sess.run中喂一组数据）</span></span><br><span class="line">x=tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">2</span>))</span><br><span class="line">w1=tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">3</span>],stddev=<span class="number">1</span>,seed=<span class="number">1</span>))</span><br><span class="line">w2=tf.Variable(tf.random_normal([<span class="number">3</span>,<span class="number">1</span>],stddev=<span class="number">1</span>,seed=<span class="number">1</span>))</span><br><span class="line"><span class="comment">#定义前向传播</span></span><br><span class="line">a=tf.matmul(x,w1);</span><br><span class="line">y=tf.matmul(a,w2);</span><br><span class="line"><span class="comment">#用会话计算结果</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">	init_op=tf.global_variables_initializer()</span><br><span class="line">	sess.run(init_op)</span><br><span class="line">	print(<span class="string">"y is :"</span>,sess.run(y,feed_dict=&#123;x:[[<span class="number">0.7</span>,<span class="number">0.5</span>],[<span class="number">0.2</span>,<span class="number">0.3</span>],[<span class="number">0.3</span>,<span class="number">0.4</span>],[<span class="number">0.4</span>,<span class="number">0.5</span>]]&#125;))</span><br><span class="line">	print(<span class="string">"w1"</span>,sess.run(w1))</span><br><span class="line">	print(<span class="string">"w2"</span>,sess.run(w2))</span><br></pre></td></tr></table></figure><p></p><p>打印出来的结果：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">y is : [[3.0904665]</span><br><span class="line"> [1.2236414]</span><br><span class="line"> [1.7270732]</span><br><span class="line"> [2.2305048]]</span><br><span class="line">w1 [[-0.8113182   1.4845988   0.06532937]</span><br><span class="line"> [-2.4427042   0.0992484   0.5912243 ]]</span><br><span class="line">w2 [[-0.8113182 ]</span><br><span class="line"> [ 1.4845988 ]</span><br><span class="line"> [ 0.06532937]]</span><br></pre></td></tr></table></figure><p></p><p>全过程代码：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="comment">#0导入模块，生成模拟数据集</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">BATCH_SIZE=<span class="number">8</span> <span class="comment">#一次喂入的数据</span></span><br><span class="line">seed=<span class="number">23455</span></span><br><span class="line"><span class="comment">#基于seed产生随机数</span></span><br><span class="line">rng=np.random.RandomState(seed)</span><br><span class="line"><span class="comment">#随机生成32行2列的输入数据集</span></span><br><span class="line">X=rng.rand(<span class="number">32</span>,<span class="number">2</span>)</span><br><span class="line">Y=[[int(x0+x1&lt;<span class="number">1</span>)]<span class="keyword">for</span>(x0,x1)inX]</span><br><span class="line">print(<span class="string">"X:"</span>,X);</span><br><span class="line">print(<span class="string">"Y:"</span>,Y);</span><br><span class="line"><span class="comment">#1定义神经网络的输入、参数和输出，定义前向传播过程</span></span><br><span class="line">x=tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">2</span>))</span><br><span class="line">y_=tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">1</span>))</span><br><span class="line">w1=tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">3</span>],stddev=<span class="number">1</span>,seed=<span class="number">1</span>))</span><br><span class="line">w2=tf.Variable(tf.random_normal([<span class="number">3</span>,<span class="number">1</span>],stddev=<span class="number">1</span>,seed=<span class="number">1</span>))</span><br><span class="line"><span class="comment">#定义前向传播</span></span><br><span class="line">a=tf.matmul(x,w1);</span><br><span class="line">y=tf.matmul(a,w2);</span><br><span class="line"><span class="comment">#2定义损失函数及反向传播方法。</span></span><br><span class="line">loss=tf.reduce_mean(tf.square(y-y\_))</span><br><span class="line">train_step=tf.train.GradientDescentOptimizer(<span class="number">0.001</span>).minimize(loss)</span><br><span class="line"><span class="comment">#train_step=tf.train.MomentumOptimizer(learning_rate).minimize(loss)</span></span><br><span class="line"><span class="comment">#train_step=tf.train.AdamOptimizer(learning_rate).minimize(loss)</span></span><br><span class="line"><span class="comment">#3生成会话，训练STEPS轮</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">	init_op=tf.global_variables_initializer()</span><br><span class="line">	sess.run(init_op)</span><br><span class="line">	<span class="comment">#输出目前（未经训练)的参数取值</span></span><br><span class="line">	print(<span class="string">"w1"</span>,sess.run(w1))</span><br><span class="line">	print(<span class="string">"w2"</span>,sess.run(w2))</span><br><span class="line">	<span class="comment">#训练模型</span></span><br><span class="line">	STEPS=<span class="number">3000</span> <span class="comment">#训练3000轮</span></span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">		start=(i*BATCH_SIZE)%<span class="number">32</span> <span class="comment">#这是因为有总共有32行</span></span><br><span class="line">		end=start+BATCH_SIZE</span><br><span class="line">		sess.run(train_step,feed_dict=&#123;x:X[start:end],y_:Y[start:end]&#125;)</span><br><span class="line">		<span class="keyword">if</span> i%<span class="number">500</span>==<span class="number">0</span>:</span><br><span class="line">			total_loss=sess.run(loss,feed_dict=&#123;x:X,y_:Y&#125;)</span><br><span class="line">			print(<span class="string">"After &#123;0&#125; traing step(s),loss on all data is &#123;1&#125;"</span>.format(i,total_loss))</span><br><span class="line">	print(<span class="string">"w1"</span>,sess.run(w1))</span><br><span class="line">	print(<span class="string">"w2"</span>,sess.run(w2))</span><br></pre></td></tr></table></figure><p></p><p>结果：可以看到损失越来越小<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">X: [[0.83494319 0.11482951]</span><br><span class="line"> [0.66899751 0.46594987]</span><br><span class="line"> [0.60181666 0.58838408]</span><br><span class="line"> [0.31836656 0.20502072]</span><br><span class="line"> [0.87043944 0.02679395]</span><br><span class="line"> [0.41539811 0.43938369]</span><br><span class="line"> [0.68635684 0.24833404]</span><br><span class="line"> [0.97315228 0.68541849]</span><br><span class="line"> [0.03081617 0.89479913]</span><br><span class="line"> [0.24665715 0.28584862]</span><br><span class="line"> [0.31375667 0.47718349]</span><br><span class="line"> [0.56689254 0.77079148]</span><br><span class="line"> [0.7321604  0.35828963]</span><br><span class="line"> [0.15724842 0.94294584]</span><br><span class="line"> [0.34933722 0.84634483]</span><br><span class="line"> [0.50304053 0.81299619]</span><br><span class="line"> [0.23869886 0.9895604 ]</span><br><span class="line"> [0.4636501  0.32531094]</span><br><span class="line"> [0.36510487 0.97365522]</span><br><span class="line"> [0.73350238 0.83833013]</span><br><span class="line"> [0.61810158 0.12580353]</span><br><span class="line"> [0.59274817 0.18779828]</span><br><span class="line"> [0.87150299 0.34679501]</span><br><span class="line"> [0.25883219 0.50002932]</span><br><span class="line"> [0.75690948 0.83429824]</span><br><span class="line"> [0.29316649 0.05646578]</span><br><span class="line"> [0.10409134 0.88235166]</span><br><span class="line"> [0.06727785 0.57784761]</span><br><span class="line"> [0.38492705 0.48384792]</span><br><span class="line"> [0.69234428 0.19687348]</span><br><span class="line"> [0.42783492 0.73416985]</span><br><span class="line"> [0.09696069 0.04883936]]</span><br><span class="line">Y: [[1], [0], [0], [1], [1], [1], [1], [0], [1], [1], [1], [0], [0], [0], [0], [0], [0], [1], [0], [0], [1], [1], [0], [1], [0], [1], [1], [1], [1], [1], [0], [1]]</span><br><span class="line">w1 [[-0.8113182   1.4845988   0.06532937]</span><br><span class="line"> [-2.4427042   0.0992484   0.5912243 ]]</span><br><span class="line">w2 [[-0.8113182 ]</span><br><span class="line"> [ 1.4845988 ]</span><br><span class="line"> [ 0.06532937]]</span><br><span class="line">After 0 traing step(s),loss on all data is 5.131181716918945</span><br><span class="line">After 500 traing step(s),loss on all data is 0.4291110336780548</span><br><span class="line">After 1000 traing step(s),loss on all data is 0.4097890853881836</span><br><span class="line">After 1500 traing step(s),loss on all data is 0.39992278814315796</span><br><span class="line">After 2000 traing step(s),loss on all data is 0.39414554834365845</span><br><span class="line">After 2500 traing step(s),loss on all data is 0.39059656858444214</span><br><span class="line">w1 [[-0.7000663   0.9136318   0.08953571]</span><br><span class="line"> [-2.3402493  -0.14641267  0.58823055]]</span><br><span class="line">w2 [[-0.06024267]</span><br><span class="line"> [ 0.91956186]</span><br><span class="line"> [-0.0682071 ]]</span><br></pre></td></tr></table></figure><p></p><h1 id="滑动平均-影子值"><a href="#滑动平均-影子值" class="headerlink" title="滑动平均(影子值)"></a>滑动平均(影子值)</h1><p>记录了每个参数一段时间内过往值得平均，增加了模型得泛化性。针对所有参数w和b(像是给参数加了影子，参数变化，影子慢慢追随)<br>$$<br>影子=衰减率×影子+(1-衰减率)×参数<br>$$<br>$$<br>影子初值=参数初值<br>$$<br>$$<br>衰减率=min\lbrace MOVING\underline{}AVERAGE\underline{}DECAY，\frac{1+轮数}{10+轮数} \rbrace<br>$$<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ema=tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)</span><br><span class="line">ema_op=ema.apply(tf.trainable_values())<span class="comment">#每运行此句，所有待优化参数求滑动平均</span></span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies([train_step,ema_op]):</span><br><span class="line">	train_op=tf.no_op(name=<span class="string">'train'</span>)</span><br></pre></td></tr></table></figure><p></p><p>代码示例:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment">#定义一个32位浮点变量，初始值为0,这个代码不断更新w1参数，优化w1参数，滑动平均就是w1的影子</span></span><br><span class="line">w1=tf.Variable(<span class="number">0</span>,dtype=tf.float32)</span><br><span class="line"><span class="comment">#定义num_updates(NN的迭代轮数)初始值为0，不可被优化（训练），这个参数不训练</span></span><br><span class="line">global_step=tf.Variable(<span class="number">0</span>,trainable=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment">#实例化滑动平均累，给删减率为0.99,当前轮数global_step</span></span><br><span class="line">MOVING_AVERAGE_DECAY=<span class="number">0.99</span></span><br><span class="line">ema=tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)</span><br><span class="line">ema_op=ema.apply(tf.trainable_variables())</span><br><span class="line"><span class="comment">#查看不同迭代中变量取值的变化</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">	init_op=tf.global_variables_initializer()</span><br><span class="line">	sess.run(init_op)</span><br><span class="line">	print(sess.run([w1,ema.average(w1)]))</span><br><span class="line">	sess.run(tf.assign(w1,<span class="number">1</span>))</span><br><span class="line">	sess.run(ema_op)</span><br><span class="line">	print(sess.run([w1,ema.average(w1)]))</span><br><span class="line">	sess.run(tf.assign(global_step,<span class="number">100</span>))</span><br><span class="line">	sess.run(tf.assign(w1,<span class="number">10</span>))</span><br><span class="line">	sess.run(ema_op)</span><br><span class="line">	print(sess.run([w1,ema.average(w1)]))</span><br><span class="line">	sess.run(ema_op)</span><br><span class="line">	print(sess.run([w1,ema.average(w1)]))</span><br><span class="line">	sess.run(ema_op)</span><br><span class="line">	print(sess.run([w1,ema.average(w1)]))</span><br></pre></td></tr></table></figure><p></p><p>运行结果：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[0.0, 0.0]</span><br><span class="line">[1.0, 0.9]</span><br><span class="line">[10.0, 1.6445453]</span><br><span class="line">[10.0, 2.3281732]</span><br><span class="line">[10.0, 2.955868]</span><br></pre></td></tr></table></figure><p></p><h1 id="正则化缓解过拟合"><a href="#正则化缓解过拟合" class="headerlink" title="正则化缓解过拟合"></a>正则化缓解过拟合</h1><p>正则化在损失函数中引入模型复杂度指标，利用给W加权值，弱化了训练数据的噪声(一般不正则化b)<br>$$<br>loss=loss(y与y\underline{})+REGULARIZER*loss(w)<br>$$<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#L1正则</span></span><br><span class="line">l1loss=tf.contrib.layers.l1_regulaizer(REGULARIZER)(w)</span><br><span class="line"><span class="comment">#L2正则</span></span><br><span class="line">l2loss=tf.contrib.layers.l2_regulaizer(REGULARIZER)(w)</span><br><span class="line"><span class="comment">#把内容加到集合对应位置</span></span><br><span class="line">tf.add_to_collection(<span class="string">'losses'</span>,l2loss)</span><br><span class="line"><span class="comment">#在原来的损失函数上加上正则项构成总损失函数</span></span><br><span class="line">loss=cem+tf.add_n(tf.get_collection(<span class="string">"losses"</span>))</span><br></pre></td></tr></table></figure><p></p><h2 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h2><p>不带正则：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">BATCH_SIZE=<span class="number">30</span></span><br><span class="line">seed=<span class="number">2</span></span><br><span class="line"><span class="comment">#基于seed产生随机数</span></span><br><span class="line">rdm=np.random.RandomState(seed)</span><br><span class="line"><span class="comment">#随机数返回300行2列的矩阵，表示300组坐标点(x0,x1)作为输入数据集</span></span><br><span class="line">X=rdm.randn(<span class="number">300</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment">#作为输入数据集的标签</span></span><br><span class="line">Y_=[int(x0*x0+x1*x1&lt;<span class="number">2</span>) <span class="keyword">for</span> (x0,x1) <span class="keyword">in</span> X]</span><br><span class="line">Y_c=[[<span class="string">'red'</span> <span class="keyword">if</span> y <span class="keyword">else</span> <span class="string">'blue'</span>] <span class="keyword">for</span> y <span class="keyword">in</span> Y_]</span><br><span class="line">X=np.vstack(X).reshape(<span class="number">-1</span>,<span class="number">2</span>)</span><br><span class="line">Y_=np.vstack(Y_).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">print(X)</span><br><span class="line">print(Y_)</span><br><span class="line">print(Y_c)</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=np.squeeze(Y_c))</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#定义神经网络的输入、参数和输出，定义前向传播过程</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(shape,regularizer)</span>:</span></span><br><span class="line">	w=tf.Variable(tf.random_normal(shape),dtype=tf.float32)</span><br><span class="line">	tf.add_to_collection(<span class="string">'losses'</span>,tf.contrib.layers.l2_regularizer(regularizer)(w))</span><br><span class="line">	<span class="keyword">return</span> w</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_biaes</span><span class="params">(shape)</span>:</span></span><br><span class="line">	b=tf.Variable(tf.constant(<span class="number">0.01</span>,shape=shape))</span><br><span class="line">	<span class="keyword">return</span> b</span><br><span class="line">x=tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">2</span>))</span><br><span class="line">y_=tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">1</span>))</span><br><span class="line">w1=get_weight([<span class="number">2</span>,<span class="number">11</span>],<span class="number">0.01</span>)</span><br><span class="line">b1=get_biaes([<span class="number">11</span>])</span><br><span class="line">y1=tf.nn.relu(tf.matmul(x,w1)+b1)</span><br><span class="line">w2=get_weight([<span class="number">11</span>,<span class="number">1</span>],<span class="number">0.01</span>)</span><br><span class="line">b2=get_biaes([<span class="number">1</span>])</span><br><span class="line">y=tf.matmul(y1,w2)+b2</span><br><span class="line"><span class="comment">#定义损失函数</span></span><br><span class="line">loss_mse=tf.reduce_mean(tf.square(y-y_))</span><br><span class="line">loss_total=loss_mse+tf.add_n(tf.get_collection(<span class="string">'losses'</span>))</span><br><span class="line">train_step=tf.train.AdamOptimizer(<span class="number">0.0001</span>).minimize(loss_mse)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">	init_op=tf.global_variables_initializer()</span><br><span class="line">	sess.run(init_op)</span><br><span class="line">	STEPS=<span class="number">40000</span></span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">		start=(i*BATCH_SIZE)%<span class="number">300</span></span><br><span class="line">		end=start+BATCH_SIZE</span><br><span class="line">		sess.run(train_step,feed_dict=&#123;x:X[start:end],y_:Y_[start:end]&#125;)</span><br><span class="line">		<span class="keyword">if</span> i%<span class="number">2000</span>==<span class="number">0</span>:</span><br><span class="line">			loss_mse_val=sess.run(loss_mse,feed_dict=&#123;x:X,y_:Y_&#125;)</span><br><span class="line">			print(loss_mse_val)</span><br><span class="line">	xx,yy=np.mgrid[<span class="number">-3</span>:<span class="number">3</span>:<span class="number">.01</span>,<span class="number">-3</span>:<span class="number">3</span>:<span class="number">.01</span>]</span><br><span class="line">	grid=np.c_[xx.ravel(),yy.ravel()]</span><br><span class="line">	probs=sess.run(y,feed_dict=&#123;x:grid&#125;)</span><br><span class="line">	probs=probs.reshape(xx.shape)</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=np.squeeze(Y_c))</span><br><span class="line">plt.contour(xx,yy,probs,levels=[<span class="number">.5</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p></p><p>分类前:<br><img src="/images/pasted-45.png" alt="upload successful"></p><p>分类后:<br><img src="/images/pasted-46.png" alt="upload successful"></p><p>可以看出边界点有明显的过拟合。</p><p>带正则：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#前面到损失函数定义的代码都一样。</span></span><br><span class="line">train_step=tf.train.AdamOptimizer(<span class="number">0.0001</span>).minimize(loss_total)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">	init_op=tf.global_variables_initializer()</span><br><span class="line">	sess.run(init_op)</span><br><span class="line">	STEPS=<span class="number">40000</span></span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">		start=(i*BATCH_SIZE)%<span class="number">300</span></span><br><span class="line">		end=start+BATCH_SIZE</span><br><span class="line">		sess.run(train_step,feed_dict=&#123;x:X[start:end],y_:Y_[start:end]&#125;)</span><br><span class="line">		<span class="keyword">if</span> i%<span class="number">2000</span>==<span class="number">0</span>:</span><br><span class="line">			loss_mse_val=sess.run(loss_total,feed_dict=&#123;x:X,y_:Y_&#125;)</span><br><span class="line">			print(loss_mse_val)</span><br><span class="line">	xx,yy=np.mgrid[<span class="number">-3</span>:<span class="number">3</span>:<span class="number">.01</span>,<span class="number">-3</span>:<span class="number">3</span>:<span class="number">.01</span>]</span><br><span class="line">	grid=np.c_[xx.ravel(),yy.ravel()]</span><br><span class="line">	probs=sess.run(y,feed_dict=&#123;x:grid&#125;)</span><br><span class="line">	probs=probs.reshape(xx.shape)</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=np.squeeze(Y_c))</span><br><span class="line">plt.contour(xx,yy,probs,levels=[<span class="number">.5</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p></p><p>带正则结果：</p><p><img src="/images/pasted-47.png" alt="upload successful"></p><p>可以看出边界明显圆滑了很多。</p><h1 id="神经网络的八股"><a href="#神经网络的八股" class="headerlink" title="神经网络的八股"></a>神经网络的八股</h1><p><strong>准备、前传、反传、迭代</strong></p><ol><li>准备。<br>Import<br>常量定义<br>生成数据集</li><li>前向传播：定义输入、参数和输出</li><li>反向传播，定义损失函数、反向传播方法</li></ol><h2 id="搭建模块化的神经网络八股"><a href="#搭建模块化的神经网络八股" class="headerlink" title="搭建模块化的神经网络八股"></a>搭建模块化的神经网络八股</h2><p><strong>前向传播就是搭建网络，设计网络结构(forward.py)</strong><br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(x,regularizer)</span>:</span></span><br><span class="line">    w=</span><br><span class="line">    b=</span><br><span class="line">    y=</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(shape,regularizer)</span>:</span></span><br><span class="line">    w=tf.Variable()</span><br><span class="line">    tf.add_to_collection(<span class="string">'losses'</span>,tf.contrib.layers.l2_regulaizer(w))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_bias</span><span class="params">(shape)</span></span></span><br><span class="line">    b=tf.Variable()</span><br><span class="line">    <span class="keyword">return</span> b</span><br></pre></td></tr></table></figure><p></p><p><strong>反向传播就是训练网络，优化网络参数(backward.py)</strong><br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">()</span>:</span></span><br><span class="line">    x=tf.placeholder()</span><br><span class="line">    y_=tf.placeholder()</span><br><span class="line">    y=forward,forward(x,REGULARIZER)</span><br><span class="line">    global_step=tf.Variable(<span class="number">0</span>,trainable=<span class="keyword">False</span>)</span><br><span class="line">    loss=</span><br><span class="line">    loss可以是交叉熵损失、均方差损失。可以加入正则化</span><br><span class="line">    learning_rate=tf.train.exponentital_decay(</span><br><span class="line">        LEARNING_RATE_BASE,</span><br><span class="line">        global_step,</span><br><span class="line">        总样本数/BATCH_SIZE,</span><br><span class="line">        LEARNING_RATE_DECAY,</span><br><span class="line">        staircase=<span class="keyword">True</span>)</span><br><span class="line">    train_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)</span><br><span class="line">    <span class="comment">#滑动平均</span></span><br><span class="line">    ema=tf.train.ExponentialMovingAverageOptimizer(MOVING_AVERAGE_DECAY,global_step)</span><br><span class="line">    ema_op=ema.apply(tf.trainable_variables())</span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([train_step,ema_op]):</span><br><span class="line">        train_op=tf.no_op(name=<span class="string">"train"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        init_op=tf.global_variables_initializer()</span><br><span class="line">        sess.run(init_op)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">            sess.run(train_step,feed_dict=&#123;x:,y_:&#125;)</span><br><span class="line">            <span class="keyword">if</span> i %轮数==<span class="number">0</span>:</span><br><span class="line">                <span class="keyword">print</span></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    backward()</span><br></pre></td></tr></table></figure><p></p><h2 id="神经网络模块化代码示例"><a href="#神经网络模块化代码示例" class="headerlink" title="神经网络模块化代码示例"></a>神经网络模块化代码示例</h2><p><strong>forward.py</strong><br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(shape,regularizer)</span>:</span></span><br><span class="line">    w=tf.Variable(tf.random_normal(shape),dtype=tf.float32)</span><br><span class="line">    tf.add_to_collection(<span class="string">'losses'</span>,tf.contrib.layers.l2_regularizer(regularizer)(w))</span><br><span class="line">    <span class="keyword">return</span> w</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_bias</span><span class="params">(shape)</span>:</span></span><br><span class="line">    b=tf.Variable(tf.constant(<span class="number">0.01</span>,shape=shape))</span><br><span class="line">    <span class="keyword">return</span> b</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(x,regularizer)</span>:</span></span><br><span class="line">    w1=get_weight([<span class="number">2</span>,<span class="number">11</span>],regularizer)</span><br><span class="line">    b1=get_bias([<span class="number">11</span>])</span><br><span class="line">    y1=tf.nn.relu(tf.matmul(x,w1)+b1)</span><br><span class="line">    w2=get_weight([<span class="number">11</span>,<span class="number">1</span>],regularizer)</span><br><span class="line">    b2=get_bias([<span class="number">1</span>])</span><br><span class="line">    y=tf.matmul(y1,w2)+b2<span class="comment">#输出层不过激活</span></span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><p></p><p>随机生成训练数据<br><strong>generated.py</strong><br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">seed=<span class="number">2</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateds</span><span class="params">()</span>:</span></span><br><span class="line">    rdm=np.random.RandomState(seed)</span><br><span class="line">    X=rdm.randn(<span class="number">300</span>,<span class="number">2</span>)</span><br><span class="line">    Y_=[int(x0*x0+x1*x1&lt;<span class="number">2</span>) <span class="keyword">for</span>(x0,x1) <span class="keyword">in</span> X]</span><br><span class="line">    Y_c=[[<span class="string">'red'</span> <span class="keyword">if</span> y <span class="keyword">else</span> <span class="string">'blue'</span>]<span class="keyword">for</span> y <span class="keyword">in</span> Y_]</span><br><span class="line">    X=np.vstack(X).reshape(<span class="number">-1</span>,<span class="number">2</span>)</span><br><span class="line">    Y_=np.vstack(Y_).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> X,Y_,Y_c</span><br></pre></td></tr></table></figure><p></p><p><strong>backward.py</strong><br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> forward</span><br><span class="line"><span class="keyword">import</span> generateds</span><br><span class="line">STEPS=<span class="number">40000</span></span><br><span class="line">BATCH_SIZE=<span class="number">30</span></span><br><span class="line">LEARNING_RATE_BASE=<span class="number">0.001</span></span><br><span class="line">LEARNING_RATE_DECAY=<span class="number">0.999</span></span><br><span class="line">REGULARIZER=<span class="number">0.01</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">()</span>:</span></span><br><span class="line">    x=tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">2</span>))</span><br><span class="line">    y_=tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">1</span>))</span><br><span class="line">    X,Y_,Y_c=generateds.generateds()</span><br><span class="line">    y=forward.forward(x,REGULARIZER)</span><br><span class="line">    global_step=tf.Variable(<span class="number">0</span>,trainable=<span class="keyword">False</span>)</span><br><span class="line">    learning_rate=tf.train.exponential_decay(LEARNING_RATE_BASE,global_step,<span class="number">300</span>/BATCH_SIZE,LEARNING_RATE_DECAY,staircase=<span class="keyword">True</span>)</span><br><span class="line">    loss_mse=tf.reduce_mean(tf.square(y-y_))</span><br><span class="line">    loss_total=loss_mse+tf.add_n(tf.get_collection(<span class="string">'losses'</span>))</span><br><span class="line">    train_step=tf.train.AdamOptimizer(learning_rate).minimize(loss_total)</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        init_op=tf.global_variables_initializer()</span><br><span class="line">        sess.run(init_op)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">            start=(i*BATCH_SIZE)%<span class="number">300</span></span><br><span class="line">            end=start+BATCH_SIZE</span><br><span class="line">            sess.run(train_step,feed_dict=&#123;x:X[start:end],y_:Y_[start:end]&#125;)</span><br><span class="line">            <span class="keyword">if</span> i%<span class="number">2000</span>==<span class="number">0</span>:</span><br><span class="line">                loss_v=sess.run(loss_total,feed_dict=&#123;x:X[start:end],y_:Y_[start:end]&#125;)</span><br><span class="line">                print(<span class="string">"%d,%f"</span>%(i,loss_v))</span><br><span class="line">        xx,yy=np.mgrid[<span class="number">-3</span>:<span class="number">3</span>:<span class="number">.01</span>,<span class="number">-3</span>:<span class="number">3</span>:<span class="number">.01</span>]</span><br><span class="line">        grid=np.c_[xx.ravel(),yy.ravel()]</span><br><span class="line">        probs=sess.run(y,feed_dict=&#123;x:grid&#125;)</span><br><span class="line">        probs=probs.reshape(xx.shape)</span><br><span class="line">    plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=np.squeeze(Y_c))</span><br><span class="line">    plt.contour(xx,yy,probs,levels=[<span class="number">.5</span>])</span><br><span class="line">    plt.show()</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    backward()</span><br></pre></td></tr></table></figure><p></p><p><strong>结果</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">0,4.618896</span><br><span class="line">2000,0.209936</span><br><span class="line">4000,0.153730</span><br><span class="line">6000,0.124145</span><br><span class="line">8000,0.111729</span><br><span class="line">10000,0.104655</span><br><span class="line">12000,0.101355</span><br><span class="line">14000,0.099677</span><br><span class="line">16000,0.096115</span><br><span class="line">18000,0.092057</span><br><span class="line">20000,0.088706</span><br><span class="line">22000,0.084267</span><br><span class="line">24000,0.081776</span><br><span class="line">26000,0.081056</span><br><span class="line">28000,0.080574</span><br><span class="line">30000,0.080001</span><br><span class="line">32000,0.079770</span><br><span class="line">34000,0.079928</span><br><span class="line">36000,0.080087</span><br><span class="line">38000,0.079957</span><br></pre></td></tr></table></figure><p></p></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div><button id="rewardButton" disable="enable" onclick='var qr=document.getElementById("QR");"none"===qr.style.display?qr.style.display="block":qr.style.display="none"'><span>打赏</span></button><div id="QR" style="display:none"><div id="wechat" style="display:inline-block"><img id="wechat_qr" src="/images/qrcode.jpg" alt="赵思佳 微信支付"><p>微信支付</p></div><div id="alipay" style="display:inline-block"><img id="alipay_qr" src="/images/alipay.jpg" alt="赵思佳 支付宝"><p>支付宝</p></div></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Python/" rel="tag"># Python</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2018/11/26/Git使用说明/" rel="next" title="Git使用说明"><i class="fa fa-chevron-left"></i> Git使用说明</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2018/12/29/Unity架构学习笔记/" rel="prev" title="Unity架构学习笔记">Unity架构学习笔记<i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="赵思佳"><p class="site-author-name" itemprop="name">赵思佳</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">13</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">10</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">9</span> <span class="site-state-item-name">标签</span></a></div></nav></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#摘要"><span class="nav-number">1.</span> <span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#基于Tensorflow的NN"><span class="nav-number">2.</span> <span class="nav-text">基于Tensorflow的NN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#张量"><span class="nav-number">2.1.</span> <span class="nav-text">张量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据类型："><span class="nav-number">2.2.</span> <span class="nav-text">数据类型：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#计算图"><span class="nav-number">2.3.</span> <span class="nav-text">计算图</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#会话（Sesson）"><span class="nav-number">2.4.</span> <span class="nav-text">会话（Sesson）</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#神经网络实现步骤"><span class="nav-number">3.</span> <span class="nav-text">神经网络实现步骤</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#前向传播"><span class="nav-number">4.</span> <span class="nav-text">前向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#参数"><span class="nav-number">4.1.</span> <span class="nav-text">参数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#前向传播搭建模型，实现推理"><span class="nav-number">4.2.</span> <span class="nav-text">前向传播搭建模型，实现推理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#推导"><span class="nav-number">4.3.</span> <span class="nav-text">推导</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#变量初始化（with结构）"><span class="nav-number">4.4.</span> <span class="nav-text">变量初始化（with结构）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#变量初始化（使用初始化函数）"><span class="nav-number">4.5.</span> <span class="nav-text">变量初始化（使用初始化函数）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#图节点运算"><span class="nav-number">4.6.</span> <span class="nav-text">图节点运算</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#前向传播代码示例"><span class="nav-number">4.7.</span> <span class="nav-text">前向传播代码示例</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#反向传播"><span class="nav-number">5.</span> <span class="nav-text">反向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#反向传播训练方法："><span class="nav-number">5.1.</span> <span class="nav-text">反向传播训练方法：</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#激活函数"><span class="nav-number">6.</span> <span class="nav-text">激活函数</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#损失函数-loss"><span class="nav-number">7.</span> <span class="nav-text">损失函数(loss)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#均方误差MSE"><span class="nav-number">7.1.</span> <span class="nav-text">均方误差MSE</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#交叉熵Cross-Entropy"><span class="nav-number">7.2.</span> <span class="nav-text">交叉熵Cross Entropy</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#学习率"><span class="nav-number">8.</span> <span class="nav-text">学习率</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#指数衰减学习率"><span class="nav-number">8.1.</span> <span class="nav-text">指数衰减学习率</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#反向传播代码示例"><span class="nav-number">8.2.</span> <span class="nav-text">反向传播代码示例:</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#滑动平均-影子值"><span class="nav-number">9.</span> <span class="nav-text">滑动平均(影子值)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#正则化缓解过拟合"><span class="nav-number">10.</span> <span class="nav-text">正则化缓解过拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#代码示例"><span class="nav-number">10.1.</span> <span class="nav-text">代码示例</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#神经网络的八股"><span class="nav-number">11.</span> <span class="nav-text">神经网络的八股</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#搭建模块化的神经网络八股"><span class="nav-number">11.1.</span> <span class="nav-text">搭建模块化的神经网络八股</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#神经网络模块化代码示例"><span class="nav-number">11.2.</span> <span class="nav-text">神经网络模块化代码示例</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2019</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">赵思佳</span></div><div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div><span class="post-meta-divider">|</span><div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script>"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script src="/lib/jquery/index.js?v=2.1.3"></script><script src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script src="/js/src/utils.js?v=5.1.4"></script><script src="/js/src/motion.js?v=5.1.4"></script><script src="/js/src/scrollspy.js?v=5.1.4"></script><script src="/js/src/post-details.js?v=5.1.4"></script><script src="/js/src/bootstrap.js?v=5.1.4"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/live2dw/lib/L2Dwidget.min.js?0c58a1486de42ac6cc1c59c7d98ae887"></script><script>L2Dwidget.init({pluginModelPath:"assets/",model:{jsonPath:"/live2dw/assets/miku.model.json"},display:{position:"right",width:150,height:300},mobile:{show:!1},log:!1,pluginJsPath:"lib/",pluginRootPath:"live2dw/",tagMode:!1})</script></body></html>